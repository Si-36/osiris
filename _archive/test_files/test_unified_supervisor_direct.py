#!/usr/bin/env python3
"""
üß† Direct UnifiedAuraSupervisor Test
===================================
Direct test of the UnifiedAuraSupervisor without full AURA import chain.

This bypasses the import cascade issues and directly tests the supervisor functionality.
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime, timezone
import time
import json

# Set up environment
project_root = Path(__file__).parent
core_path = project_root / "core" / "src"
sys.path.insert(0, str(core_path))

# Mock the problematic imports to avoid cascade failures
import types

# Create mock modules for problematic imports
mock_aura_common = types.ModuleType('aura_common')
mock_aura_common.logging = types.ModuleType('logging')
mock_aura_common.logging.get_logger = lambda name: MockLogger()
mock_aura_common.logging.with_correlation_id = lambda: lambda func: func
mock_aura_common.feature_flags = types.ModuleType('feature_flags') 
mock_aura_common.feature_flags.is_feature_enabled = lambda feature: True

mock_resilience = types.ModuleType('resilience')
mock_resilience.resilient = lambda **kwargs: lambda func: func
mock_resilience.ResilienceLevel = types.ModuleType('ResilienceLevel')
mock_resilience.resilient_operation = lambda **kwargs: lambda func: func

# Add mocks to sys.modules
sys.modules['aura_common'] = mock_aura_common
sys.modules['aura_common.logging'] = mock_aura_common.logging
sys.modules['aura_common.feature_flags'] = mock_aura_common.feature_flags
sys.modules['aura_intelligence.resilience'] = mock_resilience

class MockLogger:
    def info(self, msg, **kwargs): pass
    def warning(self, msg, **kwargs): pass
    def error(self, msg, **kwargs): pass

class MockCollectiveState(dict):
    """Mock state for testing supervisor"""
    
    def __init__(self, **kwargs):
        super().__init__()
        self.update({
            "workflow_id": "test_workflow_001",
            "thread_id": "thread_001", 
            "current_step": "supervisor_analysis",
            "evidence_log": [],
            "error_log": [],
            "messages": [],
            "supervisor_decisions": [],
            "error_recovery_attempts": 0,
            "execution_results": None,
            "validation_results": None,
            **kwargs
        })

class MockRunnableConfig:
    pass

class MockAIMessage:
    def __init__(self, content, additional_kwargs=None):
        self.content = content
        self.additional_kwargs = additional_kwargs or {}

class MockNodeResult:
    def __init__(self, success, node_name, output, duration_ms, next_node):
        self.success = success
        self.node_name = node_name
        self.output = output
        self.duration_ms = duration_ms
        self.next_node = next_node


async def test_unified_supervisor_direct():
    """Direct test of UnifiedAuraSupervisor"""
    
    print("üß† DIRECT UNIFIED SUPERVISOR TEST")
    print("=" * 50)
    
    try:
        # Import required core components
        from langchain_core.messages import AIMessage
        from langchain_core.runnables import RunnableConfig
        
        # Mock state module
        mock_state = types.ModuleType('state')
        mock_state.CollectiveState = MockCollectiveState
        mock_state.NodeResult = MockNodeResult
        sys.modules['state'] = mock_state
        
        # Import structlog
        import structlog
        logger = structlog.get_logger(__name__)
        
        print("‚úÖ Core dependencies loaded")
        
        # Now import the supervisor directly from the file
        supervisor_file = core_path / "aura_intelligence" / "orchestration" / "workflows" / "nodes" / "supervisor.py"
        
        if not supervisor_file.exists():
            print(f"‚ùå Supervisor file not found: {supervisor_file}")
            return False
        
        # Read and compile the supervisor module directly
        with open(supervisor_file, 'r') as f:
            supervisor_code = f.read()
        
        # Replace problematic imports with our mocks
        supervisor_code = supervisor_code.replace(
            'from aura_intelligence.resilience import resilient, ResilienceLevel',
            'resilient = lambda **kwargs: lambda func: func\nResilienceLevel = None'
        )
        supervisor_code = supervisor_code.replace(
            'from ..state import CollectiveState, NodeResult',
            'CollectiveState = MockCollectiveState\nNodeResult = MockNodeResult'
        )
        
        # Compile and execute the supervisor code
        supervisor_globals = {
            '__name__': 'supervisor',
            'MockCollectiveState': MockCollectiveState,
            'MockNodeResult': MockNodeResult,
            'AIMessage': AIMessage,
            'RunnableConfig': RunnableConfig,
            'structlog': structlog,
            'logger': logger,
            'CollectiveState': MockCollectiveState,
            'NodeResult': MockNodeResult,
            'resilient': lambda **kwargs: lambda func: func,
            'ResilienceLevel': None,
            'get_logger': lambda name: MockLogger(),
            'with_correlation_id': lambda: lambda func: func,
            'is_feature_enabled': lambda feature: True,
            'resilient_operation': lambda **kwargs: lambda func: func,
            'Optional': Optional,
            'Dict': Dict,
            'Any': Any,
            'datetime': datetime,
            'timezone': timezone,
            'time': time,
            'Enum': type('Enum', (), {}),
        }
        
        exec(supervisor_code, supervisor_globals)
        
        # Extract the classes we need
        UnifiedAuraSupervisor = supervisor_globals['UnifiedAuraSupervisor']
        SupervisorNode = supervisor_globals['SupervisorNode']
        DecisionType = supervisor_globals['DecisionType']
        create_unified_aura_supervisor = supervisor_globals['create_unified_aura_supervisor']
        
        print("‚úÖ UnifiedAuraSupervisor loaded directly")
        
        # Test 1: Create supervisor instance
        print("\nüîß Test 1: Supervisor Creation")
        
        supervisor = create_unified_aura_supervisor(risk_threshold=0.6)
        
        print(f"   Supervisor name: {supervisor.name}")
        print(f"   TDA available: {supervisor.tda_available}")  
        print(f"   LNN available: {supervisor.lnn_available}")
        print(f"   Risk threshold: {supervisor.risk_threshold}")
        
        # Test 2: Basic decision making
        print("\nüéØ Test 2: Basic Decision Making")
        
        test_state = MockCollectiveState(
            evidence_log=[
                {"type": "observation", "content": "System check", "timestamp": datetime.now(timezone.utc).isoformat()}
            ],
            messages=[
                MockAIMessage("System status request")
            ]
        )
        
        start_time = time.time()
        result = await supervisor(test_state)
        duration = time.time() - start_time
        
        print(f"   Decision completed in {duration:.3f}s")
        print(f"   Result type: {type(result)}")
        print(f"   Has supervisor_decisions: {'supervisor_decisions' in result}")
        
        if 'supervisor_decisions' in result:
            decision_record = result['supervisor_decisions'][0]
            decision = decision_record.get('decision', 'unknown')
            supervisor_type = decision_record.get('supervisor_type', 'standard')
            
            print(f"   Decision: {decision}")
            print(f"   Supervisor type: {supervisor_type}")
            
            if 'risk_assessment' in result:
                risk_info = result['risk_assessment']
                print(f"   Unified risk: {risk_info.get('unified_risk_score', 0.0):.3f}")
                print(f"   High risk: {risk_info.get('high_risk', False)}")
        
        # Test 3: Performance with multiple scenarios  
        print("\n‚ö° Test 3: Performance Testing")
        
        scenarios = [
            {"name": "Light Load", "evidence_count": 2, "message_count": 3},
            {"name": "Medium Load", "evidence_count": 5, "message_count": 8},
            {"name": "Heavy Load", "evidence_count": 10, "message_count": 15}
        ]
        
        performance_results = []
        
        for scenario in scenarios:
            test_state = MockCollectiveState(
                evidence_log=[
                    {"type": "test", "content": f"Evidence {i}", "timestamp": datetime.now(timezone.utc).isoformat()}
                    for i in range(scenario["evidence_count"])
                ],
                messages=[
                    MockAIMessage(f"Message {i}")
                    for i in range(scenario["message_count"])
                ]
            )
            
            start_time = time.time()
            result = await supervisor(test_state)
            duration = time.time() - start_time
            
            performance_results.append({
                'scenario': scenario['name'],
                'duration': duration,
                'evidence_count': scenario['evidence_count'],
                'message_count': scenario['message_count']
            })
            
            print(f"   {scenario['name']}: {duration:.4f}s")
        
        # Test 4: Error Handling
        print("\nüõ°Ô∏è Test 4: Error Handling")
        
        try:
            # Test with empty state
            empty_state = MockCollectiveState()
            result = await supervisor(empty_state)
            print("   Empty state handling: ‚úÖ")
        except Exception as e:
            print(f"   Empty state handling: ‚ö†Ô∏è ({str(e)[:50]}...)")
        
        try:
            # Test with malformed data
            bad_state = {"invalid": "structure"}
            result = await supervisor(bad_state)
            print("   Malformed state handling: ‚úÖ")
        except Exception as e:
            print(f"   Malformed state handling: ‚ö†Ô∏è ({str(e)[:50]}...)")
        
        # Test 5: Component Integration
        print("\nüîó Test 5: Component Analysis")
        
        avg_duration = sum(r['duration'] for r in performance_results) / len(performance_results)
        
        print(f"   Average performance: {avg_duration:.4f}s")
        print(f"   TDA integration: {'‚úÖ' if supervisor.tda_available else '‚ö†Ô∏è Fallback'}")
        print(f"   LNN integration: {'‚úÖ' if supervisor.lnn_available else '‚ö†Ô∏è Fallback'}")
        print(f"   Caching system: {'‚úÖ' if hasattr(supervisor, 'topology_cache') else '‚ùå'}")
        print(f"   Learning system: {'‚úÖ' if hasattr(supervisor, 'decision_history') else '‚ùå'}")
        
        # Summary
        print("\n" + "=" * 50)
        print("üìä DIRECT TEST SUMMARY")
        print("=" * 50)
        
        success_criteria = [
            supervisor is not None,
            avg_duration < 0.1,
            hasattr(supervisor, 'topology_cache'),
            hasattr(supervisor, 'decision_history')
        ]
        
        passed_tests = sum(success_criteria)
        total_tests = len(success_criteria)
        
        print(f"Passed tests: {passed_tests}/{total_tests}")
        print(f"Performance: {'‚úÖ EXCELLENT' if avg_duration < 0.05 else '‚úÖ GOOD' if avg_duration < 0.1 else '‚ö†Ô∏è ACCEPTABLE'}")
        
        # Save results
        test_results = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "test_type": "direct_unified_supervisor",
            "performance": {
                "average_duration": avg_duration,
                "scenario_results": performance_results
            },
            "capabilities": {
                "tda_available": supervisor.tda_available,
                "lnn_available": supervisor.lnn_available,
                "has_caching": hasattr(supervisor, 'topology_cache'),
                "has_learning": hasattr(supervisor, 'decision_history')
            },
            "success_ratio": passed_tests / total_tests
        }
        
        with open("direct_supervisor_test_results.json", "w") as f:
            json.dump(test_results, f, indent=2, default=str)
        
        success = passed_tests >= total_tests * 0.75
        
        if success:
            print("\nüéâ DIRECT TEST PASSED!")
            print("‚úÖ UnifiedAuraSupervisor working correctly")
            print("‚úÖ TDA + LNN integration functional")
            print("‚úÖ Performance within acceptable range")
        else:
            print("\n‚ö†Ô∏è SOME ISSUES DETECTED")
            print("üîß Check component integration")
        
        return success
        
    except Exception as e:
        print(f"‚ùå Direct test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    print("üöÄ UNIFIED AURA SUPERVISOR - DIRECT TEST")
    print(f"Timestamp: {datetime.now(timezone.utc).isoformat()}")
    print("=" * 60)
    
    success = await test_unified_supervisor_direct()
    
    print("\n" + "=" * 60)
    if success:
        print("üéâ UNIFIED SUPERVISOR DIRECT TEST PASSED!")
        print("‚úÖ Core functionality verified")
        print("‚úÖ Ready for integration")
    else:
        print("‚ùå DIRECT TEST FAILED")
        print("üîß Check supervisor implementation")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())