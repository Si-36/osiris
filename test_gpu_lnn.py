"""
‚ö° Test GPU-Optimized LNN Performance
====================================

Comprehensive benchmarks for GPU-accelerated Liquid Neural Networks.
"""

import torch
import torch.nn as nn
import time
import numpy as np
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns


def test_gpu_lnn():
    """Test GPU-optimized LNN implementation"""
    print("\n" + "="*60)
    print("‚ö° GPU-OPTIMIZED LNN BENCHMARK")
    print("="*60)
    
    # Import implementations
    try:
        # Add path
        import sys
        sys.path.insert(0, '/workspace/core/src')
        
        from aura_intelligence.lnn.gpu_optimized_lnn import (
            create_gpu_lnn, GPULNNConfig, StreamingGPULNN
        )
        from aura_intelligence.lnn.core import LiquidNeuralNetwork, LiquidConfig
        
        print("‚úÖ Modules imported successfully")
        
    except Exception as e:
        print(f"‚ùå Import error: {e}")
        return
    
    # Check GPU availability
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nüñ•Ô∏è  Device: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Test configurations
    configs = [
        ("Small", {"input": 64, "output": 32, "hidden": 128, "layers": 2}),
        ("Medium", {"input": 128, "output": 64, "hidden": 256, "layers": 3}),
        ("Large", {"input": 256, "output": 128, "hidden": 512, "layers": 4}),
    ]
    
    batch_sizes = [1, 16, 32, 64, 128]
    seq_lengths = [10, 50, 100]
    
    results = []
    
    print("\nüî¨ Running Benchmarks...")
    print("-" * 60)
    
    for config_name, config in configs:
        print(f"\nüìä Configuration: {config_name}")
        print(f"   Input: {config['input']}, Hidden: {config['hidden']}, Output: {config['output']}")
        
        # Create models
        gpu_model = create_gpu_lnn(
            config['input'],
            config['output'],
            hidden_size=config['hidden'],
            num_layers=config['layers'],
            use_jit=True,
            use_mixed_precision=True
        ).to(device)
        
        cpu_config = LiquidConfig(
            hidden_sizes=[config['hidden']] * config['layers'],
            compile_mode=None
        )
        cpu_model = LiquidNeuralNetwork(
            config['input'],
            config['output'],
            cpu_config
        ).to('cpu')
        
        # Benchmark different batch sizes and sequence lengths
        for batch_size in batch_sizes:
            for seq_len in seq_lengths:
                # Skip very large configurations
                if batch_size * seq_len * config['hidden'] > 1e7:
                    continue
                
                # Create test input
                x_gpu = torch.randn(batch_size, seq_len, config['input']).to(device)
                x_cpu = x_gpu.cpu()
                
                # Warmup
                for _ in range(5):
                    _ = gpu_model(x_gpu)
                    _ = cpu_model(x_cpu)
                
                # GPU timing
                torch.cuda.synchronize() if device.type == 'cuda' else None
                start = time.time()
                for _ in range(20):
                    _ = gpu_model(x_gpu)
                torch.cuda.synchronize() if device.type == 'cuda' else None
                gpu_time = (time.time() - start) / 20
                
                # CPU timing
                start = time.time()
                for _ in range(20):
                    _ = cpu_model(x_cpu)
                cpu_time = (time.time() - start) / 20
                
                # Calculate metrics
                speedup = cpu_time / gpu_time
                throughput = batch_size * seq_len / gpu_time
                
                result = {
                    'config': config_name,
                    'batch_size': batch_size,
                    'seq_len': seq_len,
                    'gpu_time_ms': gpu_time * 1000,
                    'cpu_time_ms': cpu_time * 1000,
                    'speedup': speedup,
                    'throughput': throughput
                }
                results.append(result)
                
                print(f"   Batch={batch_size:3d}, Seq={seq_len:3d}: "
                      f"GPU={gpu_time*1000:6.2f}ms, CPU={cpu_time*1000:6.2f}ms, "
                      f"Speedup={speedup:5.1f}x")
    
    # Test streaming mode
    print("\nüåä Testing Streaming Mode...")
    streaming_model = StreamingGPULNN(128, 64, GPULNNConfig(hidden_size=256)).to(device)
    
    # Single step timing
    x_stream = torch.randn(32, 128).to(device)
    streaming_model.reset_state()
    
    # Warmup
    for _ in range(10):
        _ = streaming_model.step(x_stream)
    
    # Time single steps
    torch.cuda.synchronize() if device.type == 'cuda' else None
    start = time.time()
    for _ in range(100):
        _ = streaming_model.step(x_stream)
    torch.cuda.synchronize() if device.type == 'cuda' else None
    stream_time = (time.time() - start) / 100
    
    print(f"   Single step latency: {stream_time*1000:.2f}ms")
    print(f"   Throughput: {32/stream_time:.0f} samples/sec")
    
    # Summary statistics
    print("\nüìà Performance Summary")
    print("-" * 60)
    
    avg_speedup = np.mean([r['speedup'] for r in results])
    max_speedup = max(r['speedup'] for r in results)
    min_speedup = min(r['speedup'] for r in results)
    
    print(f"Average Speedup: {avg_speedup:.1f}x")
    print(f"Maximum Speedup: {max_speedup:.1f}x")
    print(f"Minimum Speedup: {min_speedup:.1f}x")
    
    # Best configurations
    print("\nüèÜ Best Configurations:")
    sorted_results = sorted(results, key=lambda x: x['speedup'], reverse=True)[:5]
    for i, r in enumerate(sorted_results, 1):
        print(f"{i}. {r['config']} (B={r['batch_size']}, S={r['seq_len']}): "
              f"{r['speedup']:.1f}x speedup")
    
    # Memory usage
    if device.type == 'cuda':
        print(f"\nüíæ GPU Memory Usage:")
        print(f"   Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB")
        print(f"   Reserved: {torch.cuda.memory_reserved() / 1e6:.1f} MB")
    
    # Feature tests
    print("\nüîß Feature Tests:")
    test_features(gpu_model, device)
    
    return results


def test_features(model, device):
    """Test specific GPU optimization features"""
    
    # Test 1: JIT compilation
    print("   ‚úì JIT Compilation: Enabled")
    
    # Test 2: Mixed precision
    with torch.cuda.amp.autocast():
        x = torch.randn(16, 10, model.input_size).to(device)
        output = model(x)
        print("   ‚úì Mixed Precision: Working")
    
    # Test 3: Dynamic sizing
    x = torch.randn(8, 20, model.input_size).to(device)
    output, info = model(x, return_info=True)
    complexity = info['complexity'].mean().item()
    print(f"   ‚úì Dynamic Complexity: {complexity:.3f}")
    
    # Test 4: Multi-scale time constants
    tau_bands = info['tau_bands']
    print(f"   ‚úì Time Constants: {tau_bands.tolist()}")
    
    # Test 5: Gradient flow
    x = torch.randn(4, 10, model.input_size, requires_grad=True).to(device)
    output = model(x)
    loss = output.sum()
    loss.backward()
    print("   ‚úì Gradient Flow: Working")


def visualize_results(results: List[Dict]):
    """Create visualization of benchmark results"""
    import pandas as pd
    
    df = pd.DataFrame(results)
    
    # Create speedup heatmap
    plt.figure(figsize=(12, 8))
    
    # Pivot for heatmap
    pivot = df.pivot_table(
        values='speedup',
        index='batch_size',
        columns='seq_len',
        aggfunc='mean'
    )
    
    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd',
                cbar_kws={'label': 'Speedup (x)'})
    plt.title('GPU Speedup vs Batch Size and Sequence Length')
    plt.tight_layout()
    plt.savefig('/workspace/lnn_gpu_speedup_heatmap.png', dpi=150)
    plt.close()
    
    # Throughput chart
    plt.figure(figsize=(10, 6))
    for config in df['config'].unique():
        config_df = df[df['config'] == config]
        plt.plot(config_df['batch_size'], config_df['throughput'], 
                marker='o', label=config)
    
    plt.xlabel('Batch Size')
    plt.ylabel('Throughput (samples/sec)')
    plt.title('LNN GPU Throughput')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('/workspace/lnn_gpu_throughput.png', dpi=150)
    plt.close()
    
    print("\nüìä Visualizations saved:")
    print("   - lnn_gpu_speedup_heatmap.png")
    print("   - lnn_gpu_throughput.png")


if __name__ == "__main__":
    results = test_gpu_lnn()
    
    if results:
        visualize_results(results)
    
    print("\n‚úÖ GPU LNN optimization complete!")