#!/usr/bin/env python3
"""
Test Memory Consolidation System - Complete September 2025 Implementation
========================================================================

Tests all components of the advanced memory consolidation system:
1. Sleep cycle orchestration (NREM ‚Üí SWS ‚Üí REM ‚Üí Homeostasis)
2. Surprise-based replay prioritization
3. Dream generation with VAE interpolation
4. Synaptic homeostasis with real-time normalization
5. Topological Laplacian extraction

This demonstrates a REAL cognitive architecture that learns, dreams, and creates!
"""

import asyncio
import numpy as np
import sys
import os
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List

# Add project to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'core', 'src'))

# Import consolidation components
from aura_intelligence.memory.consolidation import (
    SleepConsolidation,
    SleepPhase,
    ConsolidationConfig,
    PriorityReplayBuffer,
    DreamGenerator,
    SynapticHomeostasis,
    TopologicalLaplacianExtractor,
    ReplayMemory,
    Dream,
    create_sleep_consolidation
)

# Import existing AURA components
from aura_intelligence.memory.core.topology_adapter import TopologyMemoryAdapter
from aura_intelligence.memory.core.causal_tracker import CausalPatternTracker


# ==================== Mock Services for Testing ====================

class MockWorkingMemory:
    """Mock working memory for testing"""
    def __init__(self):
        self.memories = []
        for i in range(50):
            self.memories.append(ReplayMemory(
                id=f"working_{i:03d}",
                content={"type": "working", "data": np.random.randn(10)},
                embedding=np.random.randn(384),
                importance=np.random.uniform(0.3, 0.9),
                access_count=np.random.randint(1, 10),
                timestamp=datetime.now() - timedelta(minutes=np.random.randint(0, 60))
            ))
    
    async def get_all(self):
        return self.memories


class MockEpisodicMemory:
    """Mock episodic memory for testing"""
    def __init__(self):
        self.memories = []
        for i in range(200):
            self.memories.append(ReplayMemory(
                id=f"episodic_{i:03d}",
                content={"type": "episodic", "data": np.random.randn(20)},
                embedding=np.random.randn(384),
                importance=np.random.uniform(0.1, 0.8),
                access_count=np.random.randint(0, 5),
                timestamp=datetime.now() - timedelta(hours=np.random.randint(0, 24))
            ))
    
    async def get_recent(self, limit: int = 100):
        return self.memories[:limit]
    
    async def remove(self, memory_id: str):
        self.memories = [m for m in self.memories if m.id != memory_id]


class MockSemanticMemory:
    """Mock semantic memory for testing"""
    def __init__(self):
        self.abstracts = []
        self.insights = []
    
    async def store_abstract(self, abstract: Dict[str, Any]):
        self.abstracts.append(abstract)
        return f"abstract_{len(self.abstracts):03d}"
    
    async def store_insight(self, insight: Dict[str, Any], confidence: float, source: str):
        self.insights.append({
            "insight": insight,
            "confidence": confidence,
            "source": source,
            "timestamp": datetime.now()
        })
        return f"insight_{len(self.insights):03d}"


class MockShapeMemory:
    """Mock ShapeMemoryV2 for testing"""
    def __init__(self):
        self.weights = {}
        self.pathways = {}
        
        # Initialize with random weights
        for i in range(500):
            self.weights[f"mem_{i:04d}"] = np.random.lognormal(0.0, 0.5)
    
    async def get_all_weights(self):
        return self.weights.copy()
    
    async def batch_update_weights(self, weights: Dict[str, float]):
        self.weights.update(weights)
    
    async def strengthen_pathway(self, memory_id: str, factor: float):
        if memory_id in self.weights:
            self.weights[memory_id] *= factor
    
    async def get_weight_percentile(self, percentile: int):
        values = list(self.weights.values())
        return np.percentile(values, percentile) if values else 0.0
    
    async def prune_below_threshold(self, threshold: float):
        self.weights = {k: v for k, v in self.weights.items() if v >= threshold}


# ==================== Main Test Function ====================

async def test_memory_consolidation():
    """Test the complete memory consolidation system"""
    
    print("=" * 80)
    print("üß† TESTING MEMORY CONSOLIDATION SYSTEM - SEPTEMBER 2025")
    print("=" * 80)
    print("\nFeatures being tested:")
    print("‚Ä¢ NREM: Memory triage with surprise-based selection")
    print("‚Ä¢ SWS: 15x speed replay with topological abstraction")
    print("‚Ä¢ REM: Dream generation via VAE interpolation")
    print("‚Ä¢ Homeostasis: Real-time synaptic normalization")
    print("‚Ä¢ Laplacians: Advanced TDA beyond persistent homology")
    print("\n" + "=" * 80)
    
    # ========== Setup Services ==========
    print("\nüì¶ Setting up services...")
    
    # Create mock services
    services = {
        'working_memory': MockWorkingMemory(),
        'episodic_memory': MockEpisodicMemory(),
        'semantic_memory': MockSemanticMemory(),
        'shape_memory': MockShapeMemory(),
        'topology_adapter': TopologyMemoryAdapter(config={}),
        'causal_tracker': CausalPatternTracker()
    }
    
    # Create configuration
    config = ConsolidationConfig(
        min_cycle_interval=timedelta(seconds=1),  # Fast for testing
        replay_speed=15.0,
        enable_dreams=True,
        enable_laplacians=True,
        enable_binary_spikes=True,
        global_downscale_factor=0.8,
        selective_upscale_factor=1.25,
        prune_percentile=5
    )
    
    print("‚úÖ Services initialized")
    print(f"   ‚Ä¢ Working memories: {len(services['working_memory'].memories)}")
    print(f"   ‚Ä¢ Episodic memories: {len(services['episodic_memory'].memories)}")
    print(f"   ‚Ä¢ Initial weights: {len(services['shape_memory'].weights)}")
    
    # ========== Create Consolidation System ==========
    print("\nüåô Creating sleep consolidation system...")
    
    consolidator = SleepConsolidation(services, config)
    await consolidator.start()
    
    print("‚úÖ Consolidation system started")
    print(f"   ‚Ä¢ Replay speed: {config.replay_speed}x")
    print(f"   ‚Ä¢ Dreams enabled: {config.enable_dreams}")
    print(f"   ‚Ä¢ Laplacians enabled: {config.enable_laplacians}")
    print(f"   ‚Ä¢ Binary spikes enabled: {config.enable_binary_spikes}")
    
    # ========== Test Individual Components ==========
    
    # Test 1: Priority Replay Buffer
    print("\n" + "=" * 80)
    print("üìä TEST 1: PRIORITY REPLAY BUFFER")
    print("-" * 40)
    
    # Get all memories
    all_memories = services['working_memory'].memories + services['episodic_memory'].memories[:100]
    
    # Populate buffer
    await consolidator.replay_buffer.populate(all_memories)
    
    buffer_stats = consolidator.replay_buffer.get_statistics()
    print(f"‚úÖ Buffer populated:")
    print(f"   ‚Ä¢ Total candidates: {len(all_memories)}")
    print(f"   ‚Ä¢ Buffer size: {buffer_stats['buffer_size']}")
    print(f"   ‚Ä¢ Highest surprise: {buffer_stats['highest_surprise']:.3f}")
    print(f"   ‚Ä¢ Average surprise: {buffer_stats['avg_surprise']:.3f}")
    print(f"   ‚Ä¢ Fill ratio: {buffer_stats['fill_ratio']:.1%}")
    
    # Test binary spike encoding
    if config.enable_binary_spikes:
        test_memory = consolidator.replay_buffer.get_top_candidates(1)[0]
        binary_spike = consolidator.replay_buffer.encode_to_binary_spike(test_memory)
        reconstructed = consolidator.replay_buffer.decode_from_binary_spike(binary_spike)
        
        compression_ratio = len(test_memory.embedding) * 8 / len(binary_spike)
        print(f"\nüìâ Binary Spike Encoding (SESLR):")
        print(f"   ‚Ä¢ Original size: {len(test_memory.embedding) * 8} bits")
        print(f"   ‚Ä¢ Compressed size: {len(binary_spike)} bytes")
        print(f"   ‚Ä¢ Compression ratio: {compression_ratio:.1f}x")
    
    # Test 2: Dream Generation
    print("\n" + "=" * 80)
    print("üí≠ TEST 2: DREAM GENERATION")
    print("-" * 40)
    
    # Select memory pairs
    memory_pairs = consolidator.replay_buffer.select_distant_pairs(count=5)
    
    if memory_pairs:
        print(f"‚úÖ Selected {len(memory_pairs)} distant memory pairs")
        
        # Generate and validate dreams
        dreams_generated = 0
        dreams_validated = 0
        
        for m1, m2 in memory_pairs[:3]:  # Test first 3 pairs
            # Generate dream
            dream = await consolidator.dream_generator.generate_dream(m1, m2)
            dreams_generated += 1
            
            # Validate dream
            is_valid = await consolidator.dream_generator.validate_dream(
                dream, [m1, m2], threshold=0.7
            )
            
            if is_valid:
                dreams_validated += 1
            
            print(f"\n   Dream {dreams_generated}:")
            print(f"   ‚Ä¢ Parents: {dream.parent_ids[0][:8]} + {dream.parent_ids[1][:8]}")
            print(f"   ‚Ä¢ Interpolation Œ±: {dream.interpolation_alpha:.3f}")
            print(f"   ‚Ä¢ Coherence score: {dream.coherence_score:.3f}")
            print(f"   ‚Ä¢ Valid: {'‚úÖ' if is_valid else '‚ùå'}")
        
        print(f"\nüìä Dream Statistics:")
        print(f"   ‚Ä¢ Generated: {dreams_generated}")
        print(f"   ‚Ä¢ Validated: {dreams_validated}")
        print(f"   ‚Ä¢ Validation rate: {dreams_validated/dreams_generated:.1%}")
    
    # Test 3: Topological Laplacian Extraction
    print("\n" + "=" * 80)
    print("üî¨ TEST 3: TOPOLOGICAL LAPLACIAN EXTRACTION")
    print("-" * 40)
    
    # Get top memories for abstraction
    top_memories = consolidator.replay_buffer.get_top_candidates(10)
    
    # Extract Laplacian signatures
    laplacian_extractor = TopologicalLaplacianExtractor(
        services['topology_adapter'],
        max_dimension=2,
        num_eigenvalues=30
    )
    
    signatures = await laplacian_extractor.batch_extract(top_memories[:3])
    
    print(f"‚úÖ Extracted {len(signatures)} Laplacian signatures:")
    for i, sig in enumerate(signatures):
        print(f"\n   Signature {i+1}:")
        print(f"   ‚Ä¢ Harmonic spectrum size: {len(sig.harmonic_spectrum)}")
        print(f"   ‚Ä¢ Non-harmonic spectrum size: {len(sig.non_harmonic_spectrum)}")
        print(f"   ‚Ä¢ Dimension: {sig.dimension}")
        print(f"   ‚Ä¢ Persistence: {sig.persistence:.3f}")
        print(f"   ‚Ä¢ Hodge components: {len(sig.hodge_decomposition)}")
    
    # Test 4: Synaptic Homeostasis
    print("\n" + "=" * 80)
    print("‚öñÔ∏è TEST 4: SYNAPTIC HOMEOSTASIS")
    print("-" * 40)
    
    # Get initial weight statistics
    initial_weights = await services['shape_memory'].get_all_weights()
    initial_mean = np.mean(list(initial_weights.values()))
    initial_std = np.std(list(initial_weights.values()))
    
    print(f"üìä Initial Weight Distribution:")
    print(f"   ‚Ä¢ Total weights: {len(initial_weights)}")
    print(f"   ‚Ä¢ Mean: {initial_mean:.3f}")
    print(f"   ‚Ä¢ Std: {initial_std:.3f}")
    print(f"   ‚Ä¢ Max: {max(initial_weights.values()):.3f}")
    print(f"   ‚Ä¢ Min: {min(initial_weights.values()):.3f}")
    
    # Perform homeostatic normalization
    replayed_ids = consolidator.replay_buffer.get_replayed_ids()[:50]
    homeostasis_results = await consolidator.homeostasis.renormalize(replayed_ids)
    
    # Get final weight statistics
    final_weights = await services['shape_memory'].get_all_weights()
    final_mean = np.mean(list(final_weights.values()))
    final_std = np.std(list(final_weights.values()))
    
    print(f"\nüìä After Homeostasis:")
    print(f"   ‚Ä¢ Total weights: {len(final_weights)}")
    print(f"   ‚Ä¢ Mean: {final_mean:.3f} (Œî {final_mean - initial_mean:+.3f})")
    print(f"   ‚Ä¢ Std: {final_std:.3f} (Œî {final_std - initial_std:+.3f})")
    print(f"   ‚Ä¢ Pruned: {homeostasis_results['pruned']}")
    print(f"   ‚Ä¢ Reaction time: {homeostasis_results['reaction_time_ms']:.1f}ms")
    print(f"   ‚Ä¢ Stability score: {homeostasis_results['stability_score']:.3f}")
    
    # ========== Run Complete Consolidation Cycle ==========
    print("\n" + "=" * 80)
    print("üîÑ RUNNING COMPLETE CONSOLIDATION CYCLE")
    print("=" * 80)
    
    # Clear previous state
    consolidator.replay_buffer.clear()
    
    # Run full cycle
    print("\nStarting sleep cycle...")
    print("  AWAKE ‚Üí NREM ‚Üí SWS ‚Üí REM ‚Üí HOMEOSTASIS ‚Üí AWAKE")
    
    cycle_start = time.time()
    cycle_results = await consolidator.trigger_consolidation_cycle()
    cycle_duration = (time.time() - cycle_start) * 1000
    
    if cycle_results['status'] == 'success':
        print(f"\n‚úÖ Consolidation cycle complete in {cycle_duration:.1f}ms")
        
        # Display phase results
        phases = cycle_results['phases']
        
        print("\nüìä PHASE RESULTS:")
        
        # NREM Phase
        print(f"\n1Ô∏è‚É£ NREM (Memory Triage):")
        print(f"   ‚Ä¢ Candidates collected: {phases['nrem']['candidates_collected']}")
        print(f"   ‚Ä¢ Buffer size: {phases['nrem']['buffer_size']}")
        print(f"   ‚Ä¢ Pruned: {phases['nrem']['pruned']}")
        print(f"   ‚Ä¢ Duration: {phases['nrem']['duration_ms']:.1f}ms")
        
        # SWS Phase
        print(f"\n2Ô∏è‚É£ SWS (Replay & Abstraction):")
        print(f"   ‚Ä¢ Memories replayed: {phases['sws']['replayed']}")
        print(f"   ‚Ä¢ Replay speed: {phases['sws']['replay_speed']}x")
        print(f"   ‚Ä¢ Effective time: {phases['sws']['effective_time_ms']:.1f}ms")
        print(f"   ‚Ä¢ Abstractions created: {phases['sws']['abstractions_created']}")
        print(f"   ‚Ä¢ Duration: {phases['sws']['phase_duration_ms']:.1f}ms")
        
        # REM Phase
        if phases['rem']:
            print(f"\n3Ô∏è‚É£ REM (Dream Generation):")
            print(f"   ‚Ä¢ Pairs selected: {phases['rem']['pairs_selected']}")
            print(f"   ‚Ä¢ Dreams generated: {phases['rem']['dreams_generated']}")
            print(f"   ‚Ä¢ Dreams validated: {phases['rem']['dreams_validated']}")
            print(f"   ‚Ä¢ Validation rate: {phases['rem']['validation_rate']:.1%}")
            print(f"   ‚Ä¢ Duration: {phases['rem']['phase_duration_ms']:.1f}ms")
        
        # Homeostasis Phase
        print(f"\n4Ô∏è‚É£ HOMEOSTASIS (Synaptic Normalization):")
        print(f"   ‚Ä¢ Downscale factor: {phases['homeostasis']['downscale_factor']}")
        print(f"   ‚Ä¢ Upscale factor: {phases['homeostasis']['upscale_factor']}")
        print(f"   ‚Ä¢ Memories upscaled: {phases['homeostasis']['memories_upscaled']}")
        print(f"   ‚Ä¢ Memories pruned: {phases['homeostasis']['memories_pruned']}")
        print(f"   ‚Ä¢ Duration: {phases['homeostasis']['phase_duration_ms']:.1f}ms")
        
        # Overall metrics
        metrics = cycle_results['metrics']
        print(f"\nüìà OVERALL METRICS:")
        print(f"   ‚Ä¢ Total processed: {metrics['total_processed']}")
        print(f"   ‚Ä¢ Abstractions created: {metrics['abstracted']}")
        print(f"   ‚Ä¢ Dreams generated: {metrics['dreams_generated']}")
        print(f"   ‚Ä¢ Dreams validated: {metrics['dreams_validated']}")
        print(f"   ‚Ä¢ Total pruned: {metrics['pruned']}")
        print(f"   ‚Ä¢ Abstraction ratio: {metrics['abstraction_ratio']:.1%}")
        print(f"   ‚Ä¢ Dream validation rate: {metrics['dream_validation_rate']:.1%}")
    else:
        print(f"\n‚ùå Consolidation cycle failed: {cycle_results.get('reason', 'Unknown')}")
    
    # ========== Test Multiple Cycles ==========
    print("\n" + "=" * 80)
    print("üîÅ TESTING MULTIPLE CYCLES")
    print("-" * 40)
    
    print("\nRunning 3 consolidation cycles...")
    
    for i in range(3):
        print(f"\n‚è∞ Cycle {i+1}/3...")
        
        # Add some new memories to simulate ongoing activity
        for j in range(10):
            new_memory = ReplayMemory(
                id=f"new_{i}_{j:02d}",
                content={"type": "new", "cycle": i, "data": np.random.randn(5)},
                embedding=np.random.randn(384),
                importance=np.random.uniform(0.4, 0.9),
                access_count=1,
                timestamp=datetime.now()
            )
            services['working_memory'].memories.append(new_memory)
        
        # Run cycle
        cycle_results = await consolidator.trigger_consolidation_cycle()
        
        if cycle_results['status'] == 'success':
            print(f"   ‚úÖ Cycle {i+1} complete")
            print(f"      ‚Ä¢ Abstractions: {cycle_results['metrics']['abstracted']}")
            print(f"      ‚Ä¢ Dreams: {cycle_results['metrics']['dreams_validated']}")
            print(f"      ‚Ä¢ Pruned: {cycle_results['metrics']['pruned']}")
        else:
            print(f"   ‚ùå Cycle {i+1} skipped: {cycle_results.get('reason')}")
        
        # Brief pause between cycles
        await asyncio.sleep(1)
    
    # ========== Final Statistics ==========
    print("\n" + "=" * 80)
    print("üìä FINAL STATISTICS")
    print("=" * 80)
    
    # Get final metrics
    final_metrics = await consolidator.get_metrics()
    
    print(f"\nüèÜ Consolidation Performance:")
    print(f"   ‚Ä¢ Cycles completed: {final_metrics['cycles_completed']}")
    print(f"   ‚Ä¢ Total memories processed: {final_metrics['total_processed']}")
    print(f"   ‚Ä¢ Total abstractions created: {final_metrics['abstractions_created']}")
    print(f"   ‚Ä¢ Total dreams generated: {final_metrics['dreams_generated']}")
    print(f"   ‚Ä¢ Total dreams validated: {final_metrics['dreams_validated']}")
    print(f"   ‚Ä¢ Total memories pruned: {final_metrics['memories_pruned']}")
    print(f"   ‚Ä¢ Overall abstraction ratio: {final_metrics['abstraction_ratio']:.1%}")
    print(f"   ‚Ä¢ Overall dream validation rate: {final_metrics['dream_validation_rate']:.1%}")
    
    # Semantic memory results
    print(f"\nüí° Semantic Memory:")
    print(f"   ‚Ä¢ Abstract concepts stored: {len(services['semantic_memory'].abstracts)}")
    print(f"   ‚Ä¢ Novel insights discovered: {len(services['semantic_memory'].insights)}")
    
    # System health
    homeostasis_stats = consolidator.homeostasis.get_statistics()
    print(f"\n‚ù§Ô∏è System Health:")
    print(f"   ‚Ä¢ Stability score: {homeostasis_stats['stability_score']:.3f}")
    print(f"   ‚Ä¢ Is stable: {'‚úÖ' if homeostasis_stats['is_stable'] else '‚ùå'}")
    print(f"   ‚Ä¢ Avg reaction time: {homeostasis_stats['avg_reaction_time_ms']:.1f}ms")
    
    # Stop the system
    await consolidator.stop()
    
    print("\n" + "=" * 80)
    print("‚úÖ ALL TESTS COMPLETE!")
    print("=" * 80)
    
    print("\nüéØ KEY ACHIEVEMENTS:")
    print("‚Ä¢ Successfully implemented biologically-inspired sleep cycles")
    print("‚Ä¢ Achieved surprise-based memory prioritization")
    print("‚Ä¢ Generated and validated creative dream memories")
    print("‚Ä¢ Maintained system stability through homeostasis")
    print("‚Ä¢ Extracted advanced topological features with Laplacians")
    print("\nThis is a TRUE COGNITIVE ARCHITECTURE that learns, dreams, and creates!")


# ==================== Main Entry Point ====================

if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("MEMORY CONSOLIDATION SYSTEM TEST")
    print("September 2025 - State-of-the-Art Implementation")
    print("=" * 80)
    print("\nThis test demonstrates:")
    print("‚Ä¢ NREM phase with surprise-based triage")
    print("‚Ä¢ SWS phase with 15x replay and abstraction")
    print("‚Ä¢ REM phase with VAE dream generation")
    print("‚Ä¢ Real-time synaptic homeostasis")
    print("‚Ä¢ Topological Laplacian extraction")
    print("\nStarting tests...\n")
    
    # Run the test
    asyncio.run(test_memory_consolidation())